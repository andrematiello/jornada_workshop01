![Python](https://img.shields.io/badge/python-3.11%2B-blue)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![License](https://img.shields.io/badge/license-MIT-green)


# Pipeline ETL Modular com Testes e Logging

## DescriÃ§Ã£o do Projeto

Este projeto implementa um pipeline ETL modular em Python, dividido em trÃªs etapas principais:

- Extract: leitura e concatenaÃ§Ã£o de mÃºltiplos arquivos Excel.
- Transform: limpeza e normalizaÃ§Ã£o dos dados, exportaÃ§Ã£o para Parquet.
- Load: leitura de Parquet e exportaÃ§Ã£o final para Excel.

ApÃ³s cada etapa, o pipeline executa testes unitÃ¡rios com `pytest`.  
No final, executa um teste de integraÃ§Ã£o que valida o sucesso global!ğŸš€ 

Todos os eventos sÃ£o registrados em um arquivo de log gerado automaticamente na pasta `docs/`.
O arquivo armazena todas as etapas da pipeline, dos testes unitÃ¡rios com nome no formato: `docs/log_YYYYMMDD_HHMMSS.log`.

---

## Como rodar o projeto:
1. Clone o repositÃ³rio: git clone https://github.com/andrematiello/jornada_workshop01
2. Acesse o dir: cd workshop
3. Instale as dependÃªncias: pip install -r requirements.txt
4. Execute o pipeline: python -m app.main

---

## Como rodar os testes separadamente:
Todos os testes usam pytest.

### Para rodar:
#### Todos de uma vez: pytest

#### Individualmente:
- Teste do extract: `pytest tests/test_extract.py`
- Teste do transform: `pytest tests/test_transform.py`
- Teste do load: `pytest tests/test_load.py`
- Teste de toda pipeline: `pytest tests/test_pipeline.py`

---

## Tecnologias utilizadas
- Python 3.11+
- pandas: manipulaÃ§Ã£o de dados.
- pyarrow: leitura e escrita Parquet.
- pytest: testes automatizados.

---

## Fluxo do Pipeline
- Extract â†’ Teste, se ok:
- Transform â†’ Teste, se ok:
- Load â†’ Teste, se ok:
- Teste Final âœ…
- Log gerado ğŸ“„
- FIM ğŸ¯

---

##  CriaÃ§Ã£o do arquivo de requirements.txt
- pandas>=1.0
- pyarrow>=9.0
- pytest>=7.0

---

## ComentÃ¡rios acerca do projeto:
### Agora vocÃª tem:
ğŸ”¹ Pipeline robusto
ğŸ”¹ Testes intermediÃ¡rios
ğŸ”¹ Logs completos
ğŸ”¹ DocumentaÃ§Ã£o top!
ğŸ”¹ requirements.txt

### Este projeto entrega um pipeline ETL completo e profissional, seguindo as melhores prÃ¡ticas de Engenharia de Dados, com foco em:
ğŸ”¹ Modularidade â€” cada etapa separada com responsabilidade Ãºnica: Extract, Transform e Load.
ğŸ”¹ Testabilidade â€” testes automatizados com pytest em cada etapa, garantindo qualidade e seguranÃ§a na evoluÃ§Ã£o do cÃ³digo.
ğŸ”¹ Observabilidade â€” sistema de logging estruturado, com geraÃ§Ã£o automÃ¡tica de arquivos de log identificados por data e hora, permitindo rastreabilidade completa de cada execuÃ§Ã£o.
ğŸ”¹ AutomaÃ§Ã£o â€” execuÃ§Ã£o sequencial e validada de todo o processo, com parada imediata em caso de falha, evitando propagaÃ§Ã£o de erros.
ğŸ”¹ DocumentaÃ§Ã£o clara â€” orientaÃ§Ãµes objetivas sobre execuÃ§Ã£o, estrutura do projeto e fluxo de dados, facilitando manutenÃ§Ã£o e escalabilidade.
ğŸ”¹ EstÃ©tica e usabilidade â€” enriquecido com emojis e mensagens amigÃ¡veis para tornar a execuÃ§Ã£o mais visual e intuitiva.

## Principais caracterÃ­sticas tÃ©cnicas:
### ğŸ”’ SeguranÃ§a e Controle:
ValidaÃ§Ã£o automatizada de cada etapa via testes unitÃ¡rios com pytest, assegurando que falhas sejam identificadas e tratadas de forma imediata e controlada.
Arquitetura defensiva: o pipeline interrompe automaticamente a execuÃ§Ã£o em caso de erro, evitando propagaÃ§Ã£o de inconsistÃªncias.

### ğŸ› ï¸ Robustez e Escalabilidade:
Estrutura modular orientada a funÃ§Ãµes especÃ­ficas, garantindo manutenibilidade e facilidade de extensÃ£o para novos requisitos ou integraÃ§Ãµes.
Logging estruturado, com timestamp de execuÃ§Ã£o e status de cada etapa, viabilizando rastreabilidade completa e facilitando auditorias.

### ğŸ“Š Observabilidade e TransparÃªncia:
Todos os eventos e operaÃ§Ãµes sÃ£o registrados em logs persistentes, gerados automaticamente e armazenados em docs/, permitindo uma visÃ£o clara da execuÃ§Ã£o e apoio a processos de compliance e forense.

### ğŸš€ Entrega de Valor:
AutomaÃ§Ã£o de todo o fluxo ETL: desde a ingestÃ£o atÃ© a exportaÃ§Ã£o dos dados tratados e validados, com garantias explÃ­citas de qualidade e confiabilidade.
MitigaÃ§Ã£o de riscos operacionais com testes intermediÃ¡rios, evitando a entrega de dados corrompidos ou incompletos.
PreparaÃ§Ã£o de dados em formatos otimizados (Parquet e Excel), prontos para anÃ¡lise, reporting ou integraÃ§Ã£o com sistemas de inteligÃªncia.

---
Projeto inspirado no workshop 01 da Jornada de Dados;
Projeto realizado com apoio de InteligÃªncia Artificial (ChatGPT);

 ## âœ…"Este pipeline nÃ£o apenas executa, mas valida, registra e garante a qualidade dos dados de ponta a ponta, conforme as melhores prÃ¡ticas de Engenharia de Dados."