![Python](https://img.shields.io/badge/python-3.11%2B-blue)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![License](https://img.shields.io/badge/license-MIT-green)


<a href="https://www.flaticon.com/free-icons/etl" title="etl icons">
  <img align="left" alt="etl" height="45" width="45" src="./assets/etl.png">
</a>

# PIPELINE ETL MODULAR SIMPLES COM TESTES E LOGS


## SOBRE O PROJETO

Este projeto possui o principal foco na construÃ§Ã£o padronizada de uma estrutura de projetos e documentaÃ§Ã£o, de modo acessÃ³rio, **portanto, com menor preocupaÃ§Ã£o com a sofisticaÃ§Ã£o do ETL**, apresenta um problema de negÃ³cio e um desafio de melhorar o processo, de modo que implementa um pipeline ETL modular em Python, dividindo-o em trÃªs etapas principais:

- Extract: leitura e concatenaÃ§Ã£o de mÃºltiplos arquivos Excel.
- Transform: limpeza e normalizaÃ§Ã£o dos dados, exportaÃ§Ã£o para Parquet.
- Load: leitura de Parquet e exportaÃ§Ã£o final para Excel.

Fora observado rigor na documentaÃ§Ã£o e estrutura do projeto.  
ApÃ³s cada etapa, o pipeline executa testes unitÃ¡rios com `pytest`.  
No final, executa um teste de integraÃ§Ã£o que valida o sucesso global!ğŸš€ 

Todos os eventos sÃ£o registrados em um arquivo de log gerado automaticamente na pasta `docs/`.  
O arquivo armazena todas as etapas da pipeline, dos testes unitÃ¡rios com nome no formato: `docs/log_YYYYMMDD_HHMMSS.log`.

## FLUXO DO PIPELINE

- Extract â†’ Teste, se ok:
- Transform â†’ Teste, se ok:
- Load â†’ Teste, se ok:
- Teste Final âœ…
- Log gerado ğŸ“„
- FIM ğŸ¯

## PROBLEMA DE NEGÃ“CIO

### ContextualizaÃ§Ã£o:

Atualmente, empresas de setores como varejo, saÃºde ou logÃ­stica precisam consolidar dados provenientes de diversas fontes (planilhas, sistemas legados, APIs, bancos de dados), muitas vezes envolvendo informaÃ§Ãµes sensÃ­veis ou estratÃ©gicas ao negÃ³cio,e essa integraÃ§Ã£o Ã© frequentemente realizada de forma manual, com processos frÃ¡geis, custosos, dificultando o mapeamento correto das informaÃ§Ãµes e gerando riscos de inconsistÃªncia.  
Esses desafios impactam negativamente a capacidade de a organizaÃ§Ã£o alinhar os dados aos requisitos estratÃ©gicos do negÃ³cio, como:  

- Ganho de eficiÃªncia operacional.  
- Melhoria na qualidade das informaÃ§Ãµes.  
- Suporte a anÃ¡lises preditivas e prescritivas.  
- Cumprimento de requisitos regulatÃ³rios.  

Portanto, como automatizar a integraÃ§Ã£o e a validaÃ§Ã£o de dados de mÃºltiplas fontes para apoiar a estratÃ©gia de negÃ³cios, de forma confiÃ¡vel, rÃ¡pida, automatizada de forma que possa promover a economia e o aperfeiÃ§oamento das funÃ§Ãµes desempenhadas, agregando valor Ã s decisÃµes operacionais e tÃ¡ticas?

### Como a sua soluÃ§Ã£o endereÃ§a esse problema:

O Projeto automatiza a extraÃ§Ã£o desses arquivos, transforma os dados com as regras de negÃ³cio especÃ­ficas (ex.: ajuste de datas, padronizaÃ§Ã£o de categorias), separando o resultado final por setores, e os armazena em formato Parquet, ainda, a soluÃ§Ã£o inclui testes automatizados para validar as transformaÃ§Ãµes antes da carga, obtendo como resultados:  

ğŸ”¹ A Ã¡rea de negÃ³cios passa a receber relatÃ³rios atualizados diariamente.  
O tempo de consolidaÃ§Ã£o de dados reduz-se em 80%.  

ğŸ”¹ Mapeamento de Dados:  
Sua soluÃ§Ã£o realiza a leitura, transformaÃ§Ã£o e padronizaÃ§Ã£o de dados, permitindo identificar, mapear e consolidar informaÃ§Ãµes dispersas.  
Isso viabiliza a criaÃ§Ã£o de um inventÃ¡rio de dados estruturado, facilitando a rastreabilidade e a conformidade com normas de seguranÃ§a e privacidade.  

ğŸ”¹ AutomaÃ§Ã£o Alinhada Ã  EstratÃ©gia:  
A automaÃ§Ã£o do pipeline, com testes unitÃ¡rios e integraÃ§Ã£o contÃ­nua, reduz a dependÃªncia de processos manuais.  
Isso suporta a estratÃ©gia organizacional de transformaÃ§Ã£o digital, promovendo escalabilidade e agilidade na entrega de insights para as Ã¡reas de negÃ³cio.  

ğŸ”¹ AgregaÃ§Ã£o de Valor:  
Ao transformar dados em formatos otimizados (como Parquet), sua soluÃ§Ã£o viabiliza anÃ¡lises mais rÃ¡pidas e eficientes.  
Isso agrega valor nÃ£o apenas na reduÃ§Ã£o de custos operacionais, mas tambÃ©m no fornecimento de informaÃ§Ãµes mais precisas e tempestivas para a tomada de decisÃ£o.  

ğŸ”¹ Levantamento e Cumprimento de Requisitos:  
A estruturaÃ§Ã£o modular (extract, transform, load) permite que os requisitos de qualidade, seguranÃ§a e performance sejam claramente definidos, testados e validados.  
Os testes automatizados com pytest asseguram que as transformaÃ§Ãµes seguem as regras de negÃ³cio, garantindo confiabilidade nas entregas.  

---

## COMEÃ‡ANDO

### PrÃ©-requisitos

1. Git e Github  
VocÃª deve ter o Git instalado em sua mÃ¡quina.  
VocÃª tambÃ©m deve ter uma conta no GitHub.  

2. Pyenv  
Ã‰ usado para gerenciar versÃµes do Python.  
[InstruÃ§Ãµes de instalaÃ§Ã£o do Pyenv aqui](https://github.com/pyenv/pyenv#installation).  
Vamos usar nesse projeto o Python 3.11.4   

3. Poetry  
Este projeto utiliza Poetry para gerenciamento de dependÃªncias.  
[InstruÃ§Ãµes de instalaÃ§Ã£o do Poetry aqui](https://python-poetry.org/docs/#installation).    

4. Execute o comando para ver a documentaÃ§Ã£o do projeto:

```bash
task doc
```

### Estrutura de arquivos
---

Estrutura bÃ¡sica de arquivos para o projeto encontra-se organizada da seguinte maneira:
```bash

```

---

### InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/andrematiello/jornada_workshop01
```

2. Acesse o diretÃ³rio workshop:

```bash
cd jornada_workshop01
```

3. Configure a versÃ£o correta do Python:
```bash
pyenv install 3.11.4
```

4. Determine a versÃ£o local (do projeto) do Python:
```bash
pyenv local 3.11.4
```

5. Configure o Poetry para usar o Python 3.11.4 e ative o ambiente virtual:
```bash
poetry env use 3.11.4
```

6. Para criar o ambiente virtual, desde a versÃ£o 2 do Poetry, segundo a documentaÃ§Ã£o oficial, o Poetry Shell nÃ£o acompanha a instalaÃ§Ã£o padrÃ£o, devendo er instalado como uma dependÃªncia [Poetry Docs](https://python-poetry.org/docs/managing-environments/#powershell):
```bash
poetry self add poetry-plugin-shell
```

7. Para ativar o ambiente virtual:
```bash
poetry shell
```

8. Instale as dependÃªncias do projeto:
```bash
poetry add pandas pyarrow pytest numpy blue ignr
```

---

### Como rodar o projeto:

1. Execute o pipeline:
```bash
python -m app.main
```

2. Verifique na pasta data/output se o arquivo foi gerado corretamente: `data/output`

---

### Como rodar os testes individualmente:

- Teste do extract:
```bash
pytest tests/test_extract.py
```

- Teste do transform:
```bash
pytest tests/test_transform.py
```

- Teste do load:
```bash
pytest tests/test_load.py
```

- Teste de toda pipeline:
```bash
pytest tests/test_pipeline.py
```

### Como rodar os testes todos de uma vez:

```bash
pytest
```

---

## TECNOLOGIAS UTILIZADAS
- Python 3.11+
- Pyenv. https://pypi.org/project/pyenv-win/
- Poetry. https://pypi.org/project/poetry/
- Git e Github

## BIBLIOTECAS UTILIZADAS
- Pandas: para manipulaÃ§Ã£o dos dados. https://pypi.org/project/pandas/
- Pyarrow: para leitura e escrita na extensÃ£o Parquet. https://pypi.org/project/pyarrow/
- Pytest: testes automatizados. https://pypi.org/project/pytest/
- Numpy: Ã© uma biblioteca para realizar cÃ¡lculos numÃ©ricos e manipulaÃ§Ã£o de dados em grande escala
- Blue: para adoÃ§Ã£o de melhores prÃ¡ticas, segundo a Pep8. https://pypi.org/project/blue/
- Ignr: para criaÃ§Ã£o automatizada prÃ©via do .gitignore. https://pypi.org/project/ignr/

---

## COMENTÃRIOS

### Agora vocÃª tem:
ğŸ”¹ Um Pipeline robusto;  
ğŸ”¹ Testes intermediÃ¡rios;  
ğŸ”¹ Logs completos;  
ğŸ”¹ DocumentaÃ§Ã£o top!ğŸ˜‰  

### Este projeto entrega um pipeline ETL completo e profissional, seguindo as boas prÃ¡ticas de Engenharia de Dados, com foco em:
ğŸ”¹ Modularidade: cada etapa separada com responsabilidade Ãºnica: Extract, Transform e Load.  
ğŸ”¹ Testabilidade: testes automatizados com pytest em cada etapa, garantindo qualidade e seguranÃ§a na evoluÃ§Ã£o do cÃ³digo.  
ğŸ”¹ Observabilidade: sistema de logging estruturado, com geraÃ§Ã£o automÃ¡tica de arquivos de log identificados por data e hora, permitindo rastreabilidade completa de cada execuÃ§Ã£o.  
ğŸ”¹ AutomaÃ§Ã£o: execuÃ§Ã£o sequencial e validada de todo o processo, com parada imediata em caso de falha, evitando propagaÃ§Ã£o de erros.  
ğŸ”¹ DocumentaÃ§Ã£o clara: orientaÃ§Ãµes objetivas sobre execuÃ§Ã£o, estrutura do projeto e fluxo de dados, facilitando manutenÃ§Ã£o e escalabilidade.  
ğŸ”¹ EstÃ©tica e usabilidade: enriquecido com emojis e mensagens amigÃ¡veis para tornar a execuÃ§Ã£o mais visual e intuitiva.  

## PRINCIPAIS CARACTERÃSTICAS TÃ‰CNICAS

### ğŸ”’ SeguranÃ§a e Controle:
ValidaÃ§Ã£o automatizada de cada etapa via testes unitÃ¡rios com pytest, assegurando que falhas sejam identificadas e tratadas de forma imediata e controlada.  
Por meio de uma arquitetura defensiva, o pipeline interrompe automaticamente a execuÃ§Ã£o em caso de erro, evitando propagaÃ§Ã£o de inconsistÃªncias.

### ğŸ› ï¸ Robustez e Escalabilidade:
Estrutura modular orientada a funÃ§Ãµes especÃ­ficas, garantindo manutenibilidade e facilidade de extensÃ£o para novos requisitos ou integraÃ§Ãµes.  
Logging estruturado, com timestamp de execuÃ§Ã£o e status de cada etapa, viabilizando rastreabilidade completa e facilitando auditorias.

### ğŸ“Š Observabilidade e TransparÃªncia:
Todos os eventos e operaÃ§Ãµes sÃ£o registrados em logs persistentes, gerados automaticamente e armazenados em `docs/`, permitindo uma visÃ£o clara da execuÃ§Ã£o e apoio a processos de compliance e forense.

### ğŸš€ Entrega de Valor:
AutomaÃ§Ã£o de todo o fluxo ETL: desde a ingestÃ£o atÃ© a exportaÃ§Ã£o dos dados tratados e validados, com garantias explÃ­citas de qualidade e confiabilidade, por meio da mitigaÃ§Ã£o de riscos operacionais com testes intermediÃ¡rios, evitando a entrega de dados corrompidos ou incompletos.  
PreparaÃ§Ã£o de dados em formatos otimizados (Parquet e Excel), prontos para anÃ¡lise, reporting ou integraÃ§Ã£o com sistemas de Business Inteligence.  

---

Projeto inspirado no workshop 01 da Jornada de Dados, com adaptaÃ§Ãµes;  
Projeto realizado com apoio de InteligÃªncia Artificial (ChatGPT);  
Para prÃ³ximas melhorias, extraÃ§Ã£o de dados reais, com limpeza e transformaÃ§Ã£o, posteriormente o load em um Data Warehouse, quem sabe uma cloud provider. Ainda, um ETL orquestrado com Apache Airflow, boas prÃ¡ticas de CI/CD.

## DÃšVIDAS, SUGESTÃ•ES OU FEEDBACKS

#### ğŸš€ AndrÃ© Matiello C. Caramanti - [matiello.andre@hotmail.com](mailto:matiello.andre@hotmail.com)

#### "Este pipeline nÃ£o apenas executa, mas valida, registra e garante a qualidade dos dados de ponta a ponta, conforme as melhores prÃ¡ticas de Engenharia de Dados."

---

## LICENSE

[MIT License](https://andrematiello.notion.site/mit-license)